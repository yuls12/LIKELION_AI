{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "r7QEgEzDoSq6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19098,
     "status": "ok",
     "timestamp": 1618978376599,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "r7QEgEzDoSq6",
    "outputId": "6b7ec320-1d75-4450-ea02-1bd16a4fba2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attended-formula",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T04:37:14.284478Z",
     "start_time": "2021-04-21T04:37:05.725448Z"
    },
    "executionInfo": {
     "elapsed": 2212,
     "status": "ok",
     "timestamp": 1618978349640,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "attended-formula"
   },
   "outputs": [],
   "source": [
    "# 필요한 모듈 임포트  \n",
    "import pandas as pd  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPooling1D, concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functional-homeless",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T04:37:16.262200Z",
     "start_time": "2021-04-21T04:37:16.191348Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1816,
     "status": "ok",
     "timestamp": 1618978414136,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "functional-homeless",
    "outputId": "f557cd59-8079-42d3-d3ee-cef13a8d2340"
   },
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "train_file = \"data/chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-involvement",
   "metadata": {
    "id": "bearing-involvement"
   },
   "source": [
    "    Pandas의 read_csv()함수를 이용해 chatbot_data.csv파일을 읽어와 \n",
    "    CNN 모델 학습시 필요한 Q(질문)와 label(감정) 데이터를 features와 labels 리스트에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phantom-question",
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1618978418913,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "phantom-question"
   },
   "outputs": [],
   "source": [
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "# 문장 내 모든 단어를 시퀀스 번호로 변환\n",
    "sequences = tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eiyarU1Gomxf",
   "metadata": {
    "id": "eiyarU1Gomxf"
   },
   "outputs": [],
   "source": [
    "corpus\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-combat",
   "metadata": {
    "id": "printable-combat"
   },
   "source": [
    "    질문 리스트(features)에서 문장을 하나씩 꺼내와 text_to_word_sequence() 함수를 이용해 단어 시퀀스를 만든다.\n",
    "    이렇게 생성된 단어 시퀀스를 말뭉치(corpus) 리스트에 저장합니다. \n",
    "    그 다음 텐서플로 토크나이저의 text_to_sequences() 함수를 이용해 문장 내 모든 단어를 시퀀스 번호로 변환 \n",
    "    우리는 변환된 시퀀스 번호를 이용해 단어 임베딩 벡터를 만들 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "necessary-montana",
   "metadata": {
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1618978423499,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "necessary-montana"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twenty-settlement",
   "metadata": {
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1618978425513,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "twenty-settlement"
   },
   "outputs": [],
   "source": [
    "# 너무 크지 않게 잡아야 한다 15-20 사이로 지정\n",
    "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences,\n",
    "                                                   maxlen=MAX_SEQ_LEN,\n",
    "                                                   padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-testing",
   "metadata": {
    "id": "focal-testing"
   },
   "source": [
    "    시퀀스 번호로 만든 벡터는 한 가지 문제가 있습니다. 바로 문장의 길이가 제각각이기 때문에 벡터 크기가 다 다릅니다. \n",
    "    CNN모델의 입력 계층은 고정된 개수의 입력 노드를 가지고 있습니다.\n",
    "    시퀀스 번호로 변환된 전체 벡터 크기를 동일하게 맞춰야 합니다. \n",
    "    예제에서는 MAX_SEQ_LEN 크기만큼 넉넉하게 늘립니다. \n",
    "\n",
    "    이 경우  MAX_SEQ_LEN 크기보다 작은 벡터에는 남는 공간이 생기는데, \n",
    "    이를 0으로 채우는 작업을 해야 합니다. 이런 일련의 과정을 패딩(𝑝𝑎𝑑𝑑𝑖𝑛𝑔)처리라고 합니다. \n",
    "\n",
    "    케라스에서는 pad_sequences() 함수를 통해 시퀀스의 패딩 처리를 손쉽게 할 수 있습니다. \n",
    "\n",
    "    pad_sequences() 함수를 사용할 때 maxlen 인자  로 시퀀스의 최대 길이를 정하는데, \n",
    "    학습시킬 문장 데이터들을 사전에 분석해 최대 몇 개의 단어 토큰으로 구성되어 있는지 파악해야 합니다. \n",
    "\n",
    "    너무 크게 잡으면 빈 공간이 많이 생겨 자원의 낭비가 발생합니다. \n",
    "    반대로 너무 작게 잡으면 입력데이터가 손상되는 상황이 발생하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "successful-northwest",
   "metadata": {
    "executionInfo": {
     "elapsed": 5980,
     "status": "ok",
     "timestamp": 1618978503755,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "successful-northwest"
   },
   "outputs": [],
   "source": [
    "# 학습용, 검증용, 테스트용 데이터셋 생성 # 학습셋:검증셋:테스트셋 = 7:2:1  \n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))  \n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)  \n",
    "val_size = int(len(padded_seqs) * 0.2)  \n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-italy",
   "metadata": {
    "id": "activated-italy"
   },
   "source": [
    "    앞에서 패딩 처리된 시퀀스(padded_seqs) 벡터 리스트와 감정(labels), 리스트 전체를 데이터셋 객체로 만듭니다. \n",
    "    그 다음 데이터를 랜덤으로 섞은 후 학습용, 검증용, 테스트용 데이터셋을 7:2:1비율로 나눠 실제 학습에 필요한 데이터셋 객체를 각각 분리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "physical-bunch",
   "metadata": {
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1618978507527,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "physical-bunch"
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tender-reserve",
   "metadata": {
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1618978512196,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "tender-reserve"
   },
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "\n",
    "# shape 인자로 입력 노드에 들어올 데이터의 형상( 𝑠ℎ𝑎𝑝𝑒 )을 지정\n",
    "# 실제 패딩 처리된 시퀀스 벡터의 크기(MAX_SEQ_LEN)로 설정합니다.\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
    "\n",
    "# 임베딩 계층은 희소 벡터를 입력받아 데이터 손실을 최소화하면서 벡터 차원이 압축되는 밀집 벡터로 변환\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE,\n",
    "                            input_length=MAX_SEQ_LEN)(input_layer)\n",
    "\n",
    "# 단어 임베딩 부분의 마지막에는 50% 확률로 Dropout()을 생성\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "# 임베딩 계층을 통해 전달된 임베딩 벡터에서 특징 추출을 하는 영역을 구현\n",
    "# 합성곱 연산 과정을 떠올려보면 필터 크기에 맞게 입력 데이터 위를 슬라이딩하게 되는데, \n",
    "# 이는 3,4,5-gram  언어 모델의 개념과 비슷\n",
    "conv1 = Conv1D(filters=128,\n",
    "               kernel_size=3,\n",
    "               padding='valid',\n",
    "               activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters=128,\n",
    "               kernel_size=4,\n",
    "               padding='valid',\n",
    "               activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters=128,\n",
    "               kernel_size=5,\n",
    "               padding='valid',\n",
    "               activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "# 3, 4, 5- gram 이후 합치기\n",
    "# 각각 병렬로 처리된 합성곱 계층의 특징맵 결과를 하나로 묶어줍니다.\n",
    "concat = concatenate([pool1, pool2, pool3])  \n",
    "\n",
    "# 완전 연결 계층을 구현\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)  \n",
    "\n",
    "# 챗봇 데이터 문장에서 3가지 클래스로 감정 분류해야 하기 때문에 출력 노드가 3개인 Dense()를 생성\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "\n",
    "# logits에서 나온 점수로 소프트맥스 계층을 통해 감정 클래스별 확률을 계산\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-acquisition",
   "metadata": {
    "id": "strange-acquisition"
   },
   "source": [
    "    문장을 감정 클래스로 분류하는 CNN모델은 전처리된 입력 데이터를 단어 임베딩 처리하는 영역과 \n",
    "    합성곱 필터와 연산을 통해 문장의 특징 정보(특징맵)을 추출하고, 평탄화(𝑓𝑙𝑎𝑡𝑡𝑒𝑛)를 하는 영역, \n",
    "    그리고 완전 연결 계층(𝑓𝑢𝑙𝑙𝑦 𝑐𝑜𝑛𝑛𝑒𝑐𝑡𝑒𝑑 𝑙𝑎𝑦𝑒𝑟)을 통해 감정별로 클래스를 분류하는 영역으로 구성되어 있습니다. \n",
    "    \n",
    "    클래스 분류 모델(𝑐𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛 𝑝𝑟𝑜𝑏𝑙𝑒𝑚)을 학습할 때 주로 손실값(𝑙𝑜𝑠𝑠)을 계산하는 함수로 sparse_categorical_crossentropy를 사용합니다. 이때 크로스 엔트로피(𝑐𝑟𝑜𝑠𝑠 𝑒𝑛𝑡𝑟𝑜𝑝𝑦)을 위해 확률값을 입력으로 사용해야 하는데 이를 위해 소프트맥스 계층이 필요합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "improved-speaking",
   "metadata": {
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1618978516793,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "improved-speaking"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JCyOMzJho6S9",
   "metadata": {
    "id": "JCyOMzJho6S9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thousand-experience",
   "metadata": {
    "id": "thousand-experience"
   },
   "source": [
    "    앞에서 정의한 계층들을 케라스 모델에 추가하는 작업을 해야합니다. \n",
    "    케라스 모델을 생성할 때 Model()을 사용하는데, 인자로는 앞서 생성한 입력 계층(input_layer)과 출력 계층(predictions)을 사용\n",
    "    모델 정의 후 실제 모델을 model.compile() 함수를 통해 CNN 모델을 컴파일 합니다.  \n",
    "    최적화(𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟) 방법에는 𝑎𝑑𝑎𝑚을, 손실 함수에는 sparse_categorical_crossentropy를 사용하도록 했습니다. \n",
    "    또한 케라스 모델을 평가할 때 정확도를 확인하기 위해 metrics에 accuracy를 사용했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floppy-clothing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72650,
     "status": "ok",
     "timestamp": 1618978593502,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "floppy-clothing",
    "outputId": "9a11cfeb-c7a8-4b54-8da8-b0006d280019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "414/414 [==============================] - 41s 20ms/step - loss: 0.9827 - accuracy: 0.4973 - val_loss: 0.6599 - val_accuracy: 0.6557\n",
      "Epoch 2/5\n",
      "414/414 [==============================] - 8s 19ms/step - loss: 0.5846 - accuracy: 0.7549 - val_loss: 0.3036 - val_accuracy: 0.9074\n",
      "Epoch 3/5\n",
      "414/414 [==============================] - 8s 19ms/step - loss: 0.3280 - accuracy: 0.8868 - val_loss: 0.1721 - val_accuracy: 0.9501\n",
      "Epoch 4/5\n",
      "414/414 [==============================] - 8s 19ms/step - loss: 0.1880 - accuracy: 0.9430 - val_loss: 0.0954 - val_accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "414/414 [==============================] - 8s 19ms/step - loss: 0.1331 - accuracy: 0.9587 - val_loss: 0.0628 - val_accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfb365fd50>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-animation",
   "metadata": {
    "id": "systematic-animation"
   },
   "source": [
    "    정의한 CNN 모델을 학습합니다. \n",
    "    첫 번째 인자에는 학습용 데이터셋을 입력하고, 두 번째 validation_data 인자에는 검증용 데이터셋을 입력합니다. \n",
    "    예제에서는 에포크값을 5로 설정했으므로 모델 학습을 5회 반복합니다. \n",
    "    verbose 인자가 1인 경우에는 모델 학습 시 진행 과정을 상세하게 보여줍니다.  \n",
    "    만약 생략을 원한다면 0으로 맞춰주시면 됩니다. \n",
    "    그리고 학습 model.fit() 함수 호출 후 마지막 에포크 단계일 때 보이는 결과 화면 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deluxe-pontiac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1618978637296,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "deluxe-pontiac",
    "outputId": "25d1894b-7a50-4b81-9619-f33b57e33ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9865\n",
      "Accuracy: 98.646361\n",
      "loss: 0.045564\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)  \n",
    "print('Accuracy: %f' % (accuracy * 100))  \n",
    "print('loss: %f' % (loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-vertex",
   "metadata": {
    "id": "exterior-vertex"
   },
   "source": [
    "    evaluate() 함수를 이용해 성능을 평가합니다. \n",
    "    evaluate() 함수의 인자로 테스트용 데이터셋을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "productive-backing",
   "metadata": {
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1618978645795,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "productive-backing"
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-object",
   "metadata": {
    "id": "loose-object"
   },
   "source": [
    "    학습이 완료된 모델을 h5 파일 포맷으로 저장합니다. \n",
    "    이후 저장된 모델 파일을 불러와 문장 데이터의 감정 분류를 할 예정입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-policy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 임포트  \n",
    "import pandas as pd  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"data/chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "\n",
    "# 문장으로부터 단어를 토큰화하고 숫자에 대응시키는 딕셔너리를 사용\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "# 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "# 문장 내 모든 단어를 시퀀스 번호로 변환\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "# 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "# 패딩 처리 / 너무 크지 않게 잡아야 한다 15-20 사이로 지정\n",
    "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences,\n",
    "                                                   maxlen=MAX_SEQ_LEN,\n",
    "                                                   padding='post')\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 나누기 위한 전처리\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))  \n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "# 학습용, 검증용, 테스트용 7:2:1\n",
    "train_size = int(len(padded_seqs) * 0.7)  \n",
    "val_size = int(len(padded_seqs) * 0.2)  \n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 나누기\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
    "\n",
    "\n",
    "# CNN 모델 정의\n",
    "\n",
    "# 입력 노드 : 입력 노드에 들어올 데이터의 형상( 𝑠ℎ𝑎𝑝𝑒 )을 실제 패딩 처리된 시퀀스 벡터의 크기(MAX_SEQ_LEN)로 설정\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
    "\n",
    "# 임베딩 계층은 희소 벡터를 입력받아 데이터 손실을 최소화하면서 벡터 차원이 압축되는 밀집 벡터로 변환\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE,\n",
    "                            input_length=MAX_SEQ_LEN)(input_layer)\n",
    "\n",
    "# 단어 임베딩 부분의 마지막에는 50% 확률로 Dropout()을 생성\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "# 임베딩 계층을 통해 전달된 임베딩 벡터에서 특징 추출을 하는 영역을 구현\n",
    "# 이는 3,4,5-gram  언어 모델의 개념과 비슷\n",
    "conv1 = Conv1D(filters=128,\n",
    "               kernel_size=3,\n",
    "               padding='valid',\n",
    "               activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPooling1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters=128,\n",
    "               kernel_size=4,\n",
    "               padding='valid',\n",
    "               activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPooling1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters=128,\n",
    "               kernel_size=5,\n",
    "               padding='valid',\n",
    "               activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "\n",
    "# 각각 병렬로 처리된 합성곱 계층의 특징맵 결과를 하나로 합친다.\n",
    "concat = concatenate([pool1, pool2, pool3])  \n",
    "\n",
    "# 완전 연결 계층을 구현\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)  \n",
    "\n",
    "# 출력 노드가 3개인 Dense()를 생성\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "\n",
    "# logits에서 나온 점수로 소프트맥스 계층을 통해 감정 클래스별 확률을 계산\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)  \n",
    "print('Accuracy: %f' % (accuracy * 100))  \n",
    "print('loss: %f' % (loss))\n",
    "\n",
    "\n",
    "# 모델 저장\n",
    "model.save('cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-length",
   "metadata": {
    "id": "conceptual-length"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "driving-improvement",
   "metadata": {
    "id": "driving-improvement"
   },
   "source": [
    "#### 감정 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "grave-minister",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T04:37:31.147537Z",
     "start_time": "2021-04-21T04:37:31.083585Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1629,
     "status": "ok",
     "timestamp": 1618978839546,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "grave-minister",
    "outputId": "46a95bfa-25bb-4a61-aa40-b3d29ad7293e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"data/chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()  \n",
    "labels = data['label'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oc22oZrPsa-B",
   "metadata": {
    "id": "oc22oZrPsa-B"
   },
   "source": [
    "        pandas의 read_csv()함수를 이용해 chatbot_data.csv 파일을 읽어와 \n",
    "        label(감정)을 분류할 Q(질문) 데이터를 features 리스트에 저장합니다.\n",
    "        labels 리스트는 CNN 모델이 예측한 분류 결과와 실제 분류값을 비교하기 위한 결과로 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "based-jenny",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T04:37:38.954284Z",
     "start_time": "2021-04-21T04:37:38.584357Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1618978841925,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "based-jenny",
    "outputId": "1696cbc9-ad51-4ed4-b5e8-c9b9bdafcf9c"
   },
   "outputs": [],
   "source": [
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()  \n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "# print(sequences)\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "# print(padded_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OIfa-oksskzA",
   "metadata": {
    "id": "OIfa-oksskzA"
   },
   "source": [
    "        앞서 불러온 질문 리스트(features)에서 한 문장씩 꺼내와 \n",
    "        text_to_word_sequence() 함수를 이용해 단어 시퀀스를 만든 후 말뭉치(corpus) 리스트에 저장합니다.\n",
    "\n",
    "        그 다음으로 텐서플로 토크나이저의 texts_to_sequences()함수를 이용해 문장 내 모든 단어를 시퀀스 번호로 변환합니다.\n",
    "\n",
    "        마지막으로 단어 시퀀스 벡터 크기를 맞추기 위해 pad_sequences()함수를 사용하여 패딩 처리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "outside-bacteria",
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1618978845974,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "outside-bacteria"
   },
   "outputs": [],
   "source": [
    "# 테스트용 데이터셋 생성\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))  \n",
    "ds = ds.shuffle(len(features))\n",
    "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-1meLECTs3hx",
   "metadata": {
    "id": "-1meLECTs3hx"
   },
   "source": [
    "        앞에서 패딩 처리한 시퀀스(padded_seqs) 벡터 리스트와 감정(labels) 리스트 전체를 데이터셋 객체로 만듭니다. \n",
    "        그 다음에는 데이터를 랜덤으로 섞은 후 테스트용 데이터셋 2,000개 뽑아내 20개씩 배치 처리합나다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ethical-supervisor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1618978847736,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "ethical-supervisor",
    "outputId": "3c34d274-0f24-45d1-c31c-af3506cb1429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 15, 128)      1715072     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 15, 128)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 13, 128)      49280       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 12, 128)      65664       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 11, 128)      82048       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 384)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          49280       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 3)            387         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            12          logits[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,961,743\n",
      "Trainable params: 1,961,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "100/100 - 0s - loss: 0.0620 - accuracy: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06204930320382118, 0.9785000085830688]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정 분류 CNN 모델 불러오기\n",
    "model = load_model('data/cnn_model.h5')  \n",
    "model.summary()\n",
    "model.evaluate(test_ds, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CJBnBcKmtCtU",
   "metadata": {
    "id": "CJBnBcKmtCtU"
   },
   "source": [
    "        케라스의 load_model() 함수를 이용해 모델 파일을 불러옵니다.\n",
    "        성공적으로 모델 파일을 불러왔다면 학습된 모델 객체를 반환합니다. \n",
    "        파일에 저장된 모델  정보를 확인하기 위해 summary() 함수를 호출하고, \n",
    "        테스트셋 데이터를 이용해 모델 성능을 평가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "boolean-approval",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1618978877874,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "boolean-approval",
    "outputId": "da0882c8-6600-43e7-c5de-4aecde2780a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
      "단어 인덱스 시퀀스 :  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
      "     0     0     0]\n",
      "문장 분류(정답) :  2\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 데이터셋의 10212번째 데이터 출력  \n",
    "print(\"단어 시퀀스 : \", corpus[10212])  \n",
    "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])  \n",
    "print(\"문장 분류(정답) : \", labels[10212])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hired-spray",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 966,
     "status": "ok",
     "timestamp": 1618978909069,
     "user": {
      "displayName": "최유리",
      "photoUrl": "",
      "userId": "16723349519808453958"
     },
     "user_tz": -540
    },
    "id": "hired-spray",
    "outputId": "f34bbc93-cfc7-4e03-86a2-dec04399beeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 예측 점수 :  [[3.4567800e-06 1.0106982e-06 9.9999559e-01]]\n",
      "감정 예측 클래스 :  [2]\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
    "picks = [10212]\n",
    "predict = model.predict(padded_seqs[picks])  \n",
    "predict_class = tf.math.argmax(predict, axis=1)\n",
    "print(\"감정 예측 점수 : \", predict)\n",
    "print(\"감정 예측 클래스 : \", predict_class.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-travel",
   "metadata": {
    "id": "irish-travel"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[NLP] 4_21 수업 (챗봇 cnn p46).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
