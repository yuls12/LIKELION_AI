{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "large-fancy",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-witch",
   "metadata": {},
   "source": [
    "    ì‹œí€€ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì•ìª½ 4ê°œì˜ ìˆ«ìê°€ ì£¼ì–´ì¡Œì„ ë•Œ, \n",
    "    ê·¸ ë‹¤ìŒì— ì˜¬ ìˆ«ìë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ê¸°  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np\n",
    "\n",
    "#ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(6):\n",
    "    # [0,1,2,3], [1,2,3,4] ê°™ì€ ì •ìˆ˜ì˜ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.  \n",
    "    lst = list(range(i,i+4))\n",
    "\n",
    "    # ìœ„ì—ì„œ êµ¬í•œ ì‹œí€€ìŠ¤ì˜ ìˆ«ìë“¤ì„ ê°ê° 10ìœ¼ë¡œ ë‚˜ëˆˆ ë‹¤ìŒ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    # SimpleRNN ì— ê° íƒ€ì„ìŠ¤í…ì— í•˜ë‚˜ì”© ìˆ«ìê°€ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œë„ í•˜ë‚˜ì”© ë¶„ë¦¬í•´ì„œ ë°°ì—´ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    X.append(list(map(lambda c: [c/10], lst)))\n",
    "\n",
    "    # ì •ë‹µì— í•´ë‹¹í•˜ëŠ” 4, 5 ë“±ì˜ ì •ìˆ˜ë¥¼ ì—­ì‹œ ìœ„ì²˜ëŸ¼ 10ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì €ì¥í•©ë‹ˆë‹¤.  \n",
    "    Y.append((i+4)/10)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "for i in range(len(X)):  \n",
    "    print(X[i], Y[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ëª¨ë¸ ì •ì˜\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=10,\n",
    "                              return_sequences=False,\n",
    "                              input_shape=[4, 1]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-warren",
   "metadata": {},
   "source": [
    "    ì¶œë ¥ì„ ìœ„í•œ Dense ë ˆì´ì–´ê°€ ë’¤ì— ì¶”ê°€ë¼ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼í•  ì ì€ input_shape ì…ë‹ˆë‹¤.  \n",
    "    ì—¬ê¸°ì„œ [4,1]ì€ ê°ê° timesteps, input_dimì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. \n",
    "    timestepsë€ ìˆœí™˜ ì‹ ê²½ë§ì´ ì…ë ¥ì— ëŒ€í•´ ê³„ì‚°ì„ ë°˜ë³µí•˜ëŠ” íšŸìˆ˜ì´ê³ , input_dimì€ ì…ë ¥ë²¡í„°ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë„¤íŠ¸ì›Œí¬ í›ˆë ¨ ë° ê²°ê³¼ í™•ì¸\n",
    "model.fit(X, Y, epochs=100, verbose=0)  \n",
    "model.save('simple_rnn_1.h5')  \n",
    "print(model.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-eclipse",
   "metadata": {},
   "source": [
    "    ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ëª¨ë¸ì€ 4 íƒ€ì„ìŠ¤í…ì— ê²°ì³ ì…ë ¥ì„ ë°›ê³ , ë§ˆì§€ë§‰ì— ì¶œë ¥ê°’ì„ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. \n",
    "    ìš°ë¦¬ê°€ ì¶”ê°€í•œ Dense ë ˆì´ì–´ì—ëŠ” ë³„ë„ì˜ í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì— â„3ì€ ë°”ë¡œ ğ‘¦3ê°€ ë©ë‹ˆë‹¤. \n",
    "    ê·¸ë¦¬ê³  ì´ ê°’ê³¼ 0.4ì™€ì˜ ì°¨ì´ê°€ ğ‘šğ‘ ğ‘’, ì¦‰ í‰ê·  ì œê³± ì˜¤ì°¨ê°€ ë©ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array([[[0.6],[0.7],[0.8],[0.9]]])))\n",
    "print(model.predict(np.array([[[-0.1],[0.0],[0.1],[0.2]]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-blanket",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-isaac",
   "metadata": {},
   "source": [
    "##### 1. simpleRNN ë‘ê°œ ìŒ“ì€ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(3000):\n",
    "    # 0~1 ì‚¬ì´ì˜ ëœë¤í•œ ìˆ«ì 100 ê°œë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    lst = np.random.rand(100)\n",
    "    #print(lst)\n",
    "    \n",
    "    # ë§ˆí‚¹í•  ìˆ«ì 2ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ë½‘ìŠµë‹ˆë‹¤.\n",
    "    idx = np.random.choice(100, 2, replace=False)\n",
    "    #print(idx)\n",
    "    \n",
    "    # ë§ˆí‚¹ ì¸ë±ìŠ¤ê°€ ì €ì¥ëœ ì›-í•« ì¸ì½”ë”© ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    zeros = np.zeros(100)\n",
    "    #print(zeros)\n",
    "    \n",
    "    zeros[idx] = 1\n",
    "    #print(zeros)\n",
    "    \n",
    "    # ë§ˆí‚¹ ì¸ë±ìŠ¤ì™€ ëœë¤í•œ ìˆ«ìë¥¼ í•©ì³ì„œ X ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    X.append(np.array(list(zip(zeros, lst))))\n",
    "    #print(X)\n",
    "    \n",
    "    # ë§ˆí‚¹ ì¸ë±ìŠ¤ê°€ 1ì¸ ê°’ë“¤ë§Œ ì„œë¡œ ê³±í•´ì„œ Y ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    Y.append(np.prod(lst[idx]))\n",
    "    #print(Y)\n",
    "\n",
    "print(X[0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•œ ê³±ì…ˆ ë¬¸ì œ ëª¨ë¸ ì •ì˜\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=30,\n",
    "                              return_sequences=True,\n",
    "                              input_shape=[100, 2]),\n",
    "    tf.keras.layers.SimpleRNN(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN ë„¤íŠ¸ì›Œí¬ í•™ìŠµ\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# 2560ê°œì˜ ë°ì´í„°ë§Œ í•™ìŠµì‹œí‚µë‹ˆë‹¤. validation ë°ì´í„°ëŠ” 20% ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê²°ê³¼ í™•ì¸\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì •í™•ë„ í™•ì¸\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560 + 5])  # 5ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "for i in range(5):\n",
    "    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:',\n",
    "          abs(prediction[i][0] - Y[2560 + i]))\n",
    "\n",
    "prediction = model.predict(X[2560:])\n",
    "fail = 0\n",
    "for i in range(len(prediction)):\n",
    "    # ì˜¤ì°¨ê°€ 0.04 ì´ìƒì´ë©´ ì˜¤ë‹µì…ë‹ˆë‹¤.\n",
    "    if abs(prediction[i][0] - Y[2560 + i]) > 0.04: fail += 1\n",
    "\n",
    "print('correctness:', (440 - fail) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-mixer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "superb-organ",
   "metadata": {},
   "source": [
    "##### 2. LSTMì‚¬ìš© ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•œ ê³±ì…ˆ ë¬¸ì œ ëª¨ë¸ ì •ì˜\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,\n",
    "                                                                       2]),\n",
    "    tf.keras.layers.LSTM(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM ë„¤íŠ¸ì›Œí¬ í•™ìŠµ\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê²°ê³¼ í™•ì¸\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì •í™•ë„ í™•ì¸\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560 + 5])\n",
    "for i in range(5):\n",
    "    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:',\n",
    "          abs(prediction[i][0] - Y[2560 + i]))\n",
    "\n",
    "prediction = model.predict(X[2560:])\n",
    "cnt = 0\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i][0] - Y[2560 + i]) > 0.04: cnt += 1\n",
    "print('correctness:', (440 - cnt) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-ceramic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "institutional-prophet",
   "metadata": {},
   "source": [
    "#### Bi-LSTM\n",
    "\n",
    "    ì‹œí€€ìŠ¤ ë˜ëŠ” ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì— íŠ¹í™”ë˜ì–´ ì€ë‹‰ì¸µì—ì„œ ê³¼ê±°ì˜ ì •ë³´ë¥¼ ê¸°ì–µ\n",
    "    ìˆœí™˜ ì‹ ê²½ë§  ì˜ êµ¬ì¡°ì  íŠ¹ì„±ìƒ ë°ì´í„°ê°€ ì…ë ¥ ìˆœìœ¼ë¡œ ì²˜ë¦¬ë˜ê¸° ë•Œë¬¸ì— ì´ì „ ì‹œì ì˜ ì •ë³´ë§Œ í™œìš©í•  ìˆ˜ ë°–ì— ì—†ë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬\n",
    "    >  ì…ë ¥ ë¬¸ì¥ì„ ì–‘ë°©í–¥ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§„ë‹¤ í•˜ë”ë¼ë„ ì •ë³´ì†ì‹¤ì—†ì´ ì²˜ë¦¬ê°€ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(30, return_sequences=True), input_shape=[100, 2]))  \n",
    "model.add(Bidirectional(LSTM(30)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer = 'adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)   \n",
    "Y = np.array(Y)\n",
    "history = model.fit(X, Y, epochs=100, verbose=1, validation_split= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "plt.plot(history.history['loss'], 'b-', label='loss')  \n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')  \n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560 + 5])\n",
    "for i in range(5):\n",
    "    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:',\n",
    "          abs(prediction[i][0] - Y[2560 + i]))\n",
    "\n",
    "prediction = model.predict(X[2560:])\n",
    "cnt = 0\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i][0] - Y[2560 + i]) > 0.04:\n",
    "        cnt += 1\n",
    "print('correctness:', (440 - cnt) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "geographic-garbage",
   "metadata": {},
   "source": [
    "#### ê°œì²´ëª… ì¸ì‹\n",
    "\n",
    "    ë¬¸ì¥ ë‚´ì— í¬í•¨ëœ ì–´ë–¤ ë‹¨ì–´ê°€ ì¸ë¬¼, ì¥ì†Œ, ë‚ ì§œ ë“±ì„ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ì¸ì§€ ì¸ì‹í•˜ëŠ” ê²ƒ\n",
    "    ê°œì²´ëª… ì¸ì‹ì€ ì±—ë´‡ì—ì„œ ë¬¸ì¥ì„ ì •í™•í•˜ê²Œ í•´ì„í•˜ê¸° ìœ„í•´ ë°˜ë“œì‹œ í•´ì•¼ í•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split  #ë°ì´í„° ë¶„í• \n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#í•™ìŠµíŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def read_file(file_name):\n",
    "    sents = []  #\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            # ë§Œì•½ ì²« ë²ˆì§¸ ê°’ì´ ; ì´ ì°¸ì´ë©°, ë‘ë²ˆì§¸ ì¤„ì˜ ì²« ë²ˆì§¸ ê°’ë„ $ ì´ ë§ìœ¼ë©´,\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []  #ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±\n",
    "                \n",
    "            #ë‘ ë²ˆì§¸ ì¡°ê±´, ì—¬ê¸°ëŠ” ê·¸ëƒ¥ ë„˜ì–´ê°€ì•¼ í•˜ë‹ˆ ìœ„ ì¡°ê±´ì„ ë‹¤ì‹œ ê°€ì ¸ì™€ ë„˜ê²¨ì¤€ë‹¤.\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "                \n",
    "            #ì„¸ ë²ˆì§¸ ì¡°ê±´, ë§Œì•½ ì²« ë²ˆì§¸ ì¤„ì´ ë„ì–´ì“°ê¸°ë¼ë©´ this_sentë¥¼ sentsì— ì €ì¥\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "                \n",
    "            #ìœ„ ì¡°ê±´ì´ ë‹¤ ì•„ë‹ˆë¼ë©´ ì¦‰, ë¶„ì„ ê²°ê³¼ë“¤ì´ë¼ë©´\n",
    "             #ì´ ê°’ë“¤ì„ this_sentì— ì €ì¥\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "\n",
    "corpus = read_file('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¬¸ì¥ê³¼ í…Œê·¸ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "sentences, tags = [], []\n",
    "#ìœ„ì—ì„œ ì¶”ì¶œí•œ corpus ë°ì´í„°ì…‹ í•œì¤„ ì”© tì— í• ë‹¹\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    #tì— í• ë‹¹ëœ corpusì˜ ë°ì´ë¥¼ ë˜ ë‚˜ëˆ  wì— í• ë‹¹\n",
    "    for w in t:\n",
    "        #ê·¸ ì¤‘ ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ì™€ ë§ˆì§€ë§‰ BIO í…Œê·¸ë¥¼ ë™ì‹œì— tagged_sentenceì— ì €ì¥\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        #ê°ê° ë”°ë¡œ ì €ì¥\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    #ìœ„ì—ì„œ ì €ì¥í•œ ê°ì²´ë“¤ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì¤€ë‹¤.\n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "print(\"ìƒ˜í”Œ í¬ê¸° : \\n\", len(sentences))\n",
    "print(\"2ë²ˆì§¸ ìƒ˜í”Œ ë¬¸ì¥ ì‹œí€€ìŠ¤ : \\n\", sentences[1])\n",
    "print(\"2ë²ˆì§¸ ìƒ˜í”Œ bio íƒœê·¸ : \\n\", tags[1])\n",
    "print(\"ìƒ˜í”Œ ë¬¸ì¥ ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´ : \", max(len(l) for l in sentences))\n",
    "print(\"ìƒ˜í”Œ ë¬¸ì¥ ì‹œí€€ìŠ¤ í‰ê·  ê¸¸ì´ : \", (sum(map(len, sentences)) / len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í† í¬ë‚˜ì´ì € ì •ì˜\n",
    "#ë¯¸ë¦¬ ì¸ë±ì‹±í•˜ì§€ ì•Šì€ ë‹¨ì–´ë“¤ì€ OOVë¡œ ì§€ì •\n",
    "sent_tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "#ìœ„ì˜ ì„¤ì •ëŒ€ë¡œ ë¬¸ìë¥¼ ì…ë ¥ë°›ì•„ ë¦¬ìŠ¤íŠ¸ì˜ í˜•íƒœë¡œ ë³€í™˜\n",
    "sent_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#í…Œê·¸ ì •ë³´ëŠ” ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì§€ ì•Šë„ë¡ ì…‹íŒ…\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False)\n",
    "#ìœ„ì˜ ì„¤ì •ëŒ€ë¡œ ë¬¸ìë¥¼ ì…ë ¥ë°›ì•„ ë¦¬ìŠ¤íŠ¸ì˜ í˜•íƒœë¡œ ë³€í™˜\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "#ë¬¸ì¥ì˜ ìˆ˜\n",
    "vocab_size = len(sent_tokenizer.word_index) + 1\n",
    "#íƒœê·¸ì˜ ìˆ˜\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"ë‹¨ì–´ ì‚¬ì „ í¬ê¸° : \", vocab_size)\n",
    "print(\"BIO íƒœê·¸ ì‚¬ì „ í¬ê¸° : \", tag_size)\n",
    "\n",
    "#í•™ìŠµìš© ë‹¨ì–´ ì‹œí€€ìŠ¤ ìƒì„± - texts_to_sequences()ëŠ” í…ìŠ¤íŠ¸ ì•ˆì˜ ë‹¨ì–´ë“¤ì„ ìˆ«ìì˜ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜\n",
    "x_train = sent_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "print(x_train[1])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = sent_tokenizer.index_word  #ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
    "index_to_ner = tag_tokenizer.index_word  #ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤ë¥¼ NERë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
    "#print(index_to_word)\n",
    "#print(index_to_ner)\n",
    "index_to_ner[0] = 'PAD'  #ì²« ë²ˆì§¸ ê°’ì´ ì˜ë¯¸ ì—†ëŠ” 'O' ì´ê¸°ì— PADì²˜ë¦¬\n",
    "max_len = 40\n",
    "print(\"before : \", x_train[1])\n",
    "print(\"before : \", y_train[1])\n",
    "#ì•„ë˜ í•¨ìˆ˜ëŠ” ê¸¸ì´ê°€ ì œê°ê¸° ë‹¤ë¥¸ ë°ì´í„° ì…‹ì— ì‚¬ìš©ë˜ë©°, paddingì„ í†µí•´ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,\n",
    "                                               padding='post',\n",
    "                                               maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train,\n",
    "                                               padding='post',\n",
    "                                               maxlen=max_len)\n",
    "print(\"after :\", x_train[1])\n",
    "print(\"after :\", y_train[1])\n",
    "\n",
    "#í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "#ì¶œë ¥ ë°ì´í„°ë¥¼ ì›-í•« ì¸ì½”ë”©\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "print(\"one-hot / y_train : \", y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ëª¨ë¸ ì •ì˜ BI-LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=30,\n",
    "              input_length=max_len,\n",
    "              mask_zero=True))\n",
    "\n",
    "# ì–‘ë°©í–¥ lstm ëª¨ë¸ì„ ì •ì˜í•  ë•Œ ì •ë°©í–¥, ì—­ë°©í–¥ lstm ê³„ì¸µì— ëª¨ë“  ì¶œë ¥ê°’ì„ ì—°ê²°í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— \n",
    "# return_sequences ì¸ìë¥¼ ë°˜ë“œì‹œ True\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        LSTM(200, return_sequences=True, dropout=0.50,\n",
    "             recurrent_dropout=0.25)))\n",
    "\n",
    "#ë˜í•œ Dense ê³„ì¸µì´ 3ì°¨ì› í…ì„œë¥¼ ì…ë ¥ë°›ì„ ìˆ˜ ìˆê²Œ í™•ì¥í•´ì•¼ í•˜ë¯€ë¡œ TimeDistributed()ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"í‰ê°€ê²°ê³¼ : \", model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì‹œí€€ìŠ¤ë¥¼ NER íƒœê·¸ë¡œ ë³€í™˜\n",
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    #ê° ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¦¬\n",
    "    for sequence in sequences:\n",
    "        temp = []\n",
    "        #ê° ë°ì´í„°ì— ëŒ€í•œ íŒ¨ë”© ì²˜ë¦¬ëœ íƒœê·¸ë³„ í™•ë¥  ë¶„ë¦¬\n",
    "        for pred in sequence:\n",
    "            #ë¶„ë¦¬í•œ ê°’ ì¤‘ ì œì¼ í° ì¸ë±ìŠ¤ë¥¼ pred_indexì— ì €ì¥\n",
    "            pred_index = np.argmax(pred)\n",
    "            #print(pred_index)\n",
    "            #ì¸ë±ìŠ¤ì—ì„œ ë‹¤ì‹œ NERë¡œ ë³€í™˜í•˜ë©° ì•ì„œ 'PAD'ì²˜ë¦¬í•œ ë¶€ë¶„ì„ ë‹¤ì‹œ ì›ë˜ì˜ 'O'ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "            temp.append(index_to_ner[pred_index].replace('PAD', 'O'))\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "y_predicted = model.predict(x_test)\n",
    "#(711,40)->model->(711,40,8(ê° NERê°ì²´ì— ëŒ€í•œ í™•ë¥ ))\n",
    "#print(y_predicted)\n",
    "\n",
    "#ë¹„êµë¥¼ ìœ„í•´ì„œ\n",
    "pred_tags = sequences_to_tag(y_predicted)  #ì˜ˆì¸¡ëœ ê°’ì— ëŒ€í•œ NER\n",
    "test_tags = sequences_to_tag(y_test)  #ì‹¤ì œ ê°’ì— ëŒ€í•œ NER\n",
    "\n",
    "#F1 ìŠ¤ì½”ì–´ ê³„ì‚° ê³¼ì •\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score : {:.1%}\".format(f1_score(test_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ìƒˆë¡œìš´ ìœ í˜•ì˜ ë¬¸ì¥ NERì˜ˆì¸¡\n",
    "word_to_index = sent_tokenizer.word_index  #ê° ë‹¨ì–´ë“¤ì„ ì¸ë±ìŠ¤ë¡œ ì „í™˜\n",
    "new_sentence = \"ì‚¼ì„±ì „ì ì£¼ê°€ëŠ” ì˜¤ëŠ˜ ìŠ¤ë§ˆíŠ¸í° ì¶œì‹œë¡œ ì˜¬ë¼ê°”ë‹¤.\".split()\n",
    "new_x = []\n",
    "for w in new_sentence:\n",
    "    try:\n",
    "        #get(key,'ë””í´íŠ¸')ëŠ” keyì— ëŒ€í•œ valueë¥¼ ë°˜í™˜, ì´ë•Œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”í‚¤ë©´ KeyErrorë¥¼ ë¶ˆëŸ¬ì¼ìœ¼í‚´\n",
    "        #ì´ë•Œ ë””í´íŠ¸ê°’ì´ ì •í•´ì§„ ê²½ìš°ë¼ë©´ ê·¸ ê°’ì„ ë°˜í™˜  #ì—¬ê¸°ì„œ 1ì´ë€ ê°’ì€ 'OOV' ì…ë‹ˆë‹¤.\n",
    "        new_x.append(word_to_index.get(w, 1))\n",
    "        \n",
    "    except KeyError:\n",
    "        new_x.append(word_to_index['OOV'])\n",
    "\n",
    "#ìœ„ì˜ ì‚¼ì„±ì „ì ì˜ˆì‹œë¥¼ ìª¼ê°œì„œ ì¸ë±ì‹±í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ\n",
    "print(\"ìƒˆë¡œìš´ ìœ í˜•ì˜ ì‹œí€€ìŠ¤ : \", new_x)\n",
    "\n",
    "#íŒ¨ë”© ì²˜ë¦¬\n",
    "new_padded_seqs = preprocessing.sequence.pad_sequences([new_x],\n",
    "                                                       padding='post',\n",
    "                                                       value=0,\n",
    "                                                       maxlen=max_len)\n",
    "\n",
    "#NER ì˜ˆì¸¡\n",
    "p = model.predict(np.array([new_padded_seqs[0]]))\n",
    "p = np.argmax(p, axis=1)\n",
    "\n",
    "#ì¶œë ¥ ê²°ê³¼ í‘œì‹œ\n",
    "print(\"{:10} {:5}\".format(\"ë‹¨ì–´\", \"ì˜ˆì¸¡ëœ NER\"))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-argument",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-omaha",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-glory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
