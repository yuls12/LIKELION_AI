{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "large-fancy",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-witch",
   "metadata": {},
   "source": [
    "    시퀀스를 구성하는 앞쪽 4개의 숫자가 주어졌을 때, \n",
    "    그 다음에 올 숫자를 예측하는 간단한 시퀀스 예측 모델을 만들기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np\n",
    "\n",
    "#시퀀스 예측 데이터 생성\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(6):\n",
    "    # [0,1,2,3], [1,2,3,4] 같은 정수의 시퀀스를 만듭니다.  \n",
    "    lst = list(range(i,i+4))\n",
    "\n",
    "    # 위에서 구한 시퀀스의 숫자들을 각각 10으로 나눈 다음 저장합니다.\n",
    "    # SimpleRNN 에 각 타임스텝에 하나씩 숫자가 들어가기 때문에 여기서도 하나씩 분리해서 배열에 저장합니다.\n",
    "    X.append(list(map(lambda c: [c/10], lst)))\n",
    "\n",
    "    # 정답에 해당하는 4, 5 등의 정수를 역시 위처럼 10으로 나눠서 저장합니다.  \n",
    "    Y.append((i+4)/10)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "for i in range(len(X)):  \n",
    "    print(X[i], Y[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시퀀스 예측 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=10,\n",
    "                              return_sequences=False,\n",
    "                              input_shape=[4, 1]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-warren",
   "metadata": {},
   "source": [
    "    출력을 위한 Dense 레이어가 뒤에 추가돼 있습니다. 여기서 주목해야할 점은 input_shape 입니다.  \n",
    "    여기서 [4,1]은 각각 timesteps, input_dim을 나타냅니다. \n",
    "    timesteps란 순환 신경망이 입력에 대해 계산을 반복하는 횟수이고, input_dim은 입력벡터의 크기를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#네트워크 훈련 및 결과 확인\n",
    "model.fit(X, Y, epochs=100, verbose=0)  \n",
    "model.save('simple_rnn_1.h5')  \n",
    "print(model.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-eclipse",
   "metadata": {},
   "source": [
    "    시퀀스 예측 모델은 4 타임스텝에 결쳐 입력을 받고, 마지막에 출력값을 다음 레이어로 반환합니다. \n",
    "    우리가 추가한 Dense 레이어에는 별도의 활성화 함수가 없기 때문에 ℎ3은 바로 𝑦3가 됩니다. \n",
    "    그리고 이 값과 0.4와의 차이가 𝑚𝑠𝑒, 즉 평균 제곱 오차가 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array([[[0.6],[0.7],[0.8],[0.9]]])))\n",
    "print(model.predict(np.array([[[-0.1],[0.0],[0.1],[0.2]]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-blanket",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-isaac",
   "metadata": {},
   "source": [
    "##### 1. simpleRNN 두개 쌓은 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i in range(3000):\n",
    "    # 0~1 사이의 랜덤한 숫자 100 개를 만듭니다.\n",
    "    lst = np.random.rand(100)\n",
    "    #print(lst)\n",
    "    \n",
    "    # 마킹할 숫자 2개의 인덱스를 뽑습니다.\n",
    "    idx = np.random.choice(100, 2, replace=False)\n",
    "    #print(idx)\n",
    "    \n",
    "    # 마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만듭니다.\n",
    "    zeros = np.zeros(100)\n",
    "    #print(zeros)\n",
    "    \n",
    "    zeros[idx] = 1\n",
    "    #print(zeros)\n",
    "    \n",
    "    # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X 에 저장합니다.\n",
    "    X.append(np.array(list(zip(zeros, lst))))\n",
    "    #print(X)\n",
    "    \n",
    "    # 마킹 인덱스가 1인 값들만 서로 곱해서 Y 에 저장합니다.\n",
    "    Y.append(np.prod(lst[idx]))\n",
    "    #print(Y)\n",
    "\n",
    "print(X[0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN 레이어를 사용한 곱셈 문제 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=30,\n",
    "                              return_sequences=True,\n",
    "                              input_shape=[100, 2]),\n",
    "    tf.keras.layers.SimpleRNN(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN 네트워크 학습\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# 2560개의 데이터만 학습시킵니다. validation 데이터는 20% 로 지정합니다.\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN 네트워크 학습 결과 확인\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 데이터에 대한 예측 정확도 확인\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560 + 5])  # 5개 테스트 데이터에 대한 예측을 표시합니다.\n",
    "for i in range(5):\n",
    "    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:',\n",
    "          abs(prediction[i][0] - Y[2560 + i]))\n",
    "\n",
    "prediction = model.predict(X[2560:])\n",
    "fail = 0\n",
    "for i in range(len(prediction)):\n",
    "    # 오차가 0.04 이상이면 오답입니다.\n",
    "    if abs(prediction[i][0] - Y[2560 + i]) > 0.04: fail += 1\n",
    "\n",
    "print('correctness:', (440 - fail) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-mixer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "superb-organ",
   "metadata": {},
   "source": [
    "##### 2. LSTM사용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 레이어를 사용한 곱셈 문제 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,\n",
    "                                                                       2]),\n",
    "    tf.keras.layers.LSTM(units=30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 네트워크 학습\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 네트워크 학습 결과 확인\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test 데이터에 대한 예측 정확도 확인\n",
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560 + 5])\n",
    "for i in range(5):\n",
    "    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:',\n",
    "          abs(prediction[i][0] - Y[2560 + i]))\n",
    "\n",
    "prediction = model.predict(X[2560:])\n",
    "cnt = 0\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i][0] - Y[2560 + i]) > 0.04: cnt += 1\n",
    "print('correctness:', (440 - cnt) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-ceramic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "institutional-prophet",
   "metadata": {},
   "source": [
    "#### Bi-LSTM\n",
    "\n",
    "    시퀀스 또는 시계열 데이터 처리에 특화되어 은닉층에서 과거의 정보를 기억\n",
    "    순환 신경망  의 구조적 특성상 데이터가 입력 순으로 처리되기 때문에 이전 시점의 정보만 활용할 수 밖에 없다는 단점이 존재\n",
    "    >  입력 문장을 양방향에서 처리하므로 시퀀스 길이가 길어진다 하더라도 정보손실없이 처리가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(30, return_sequences=True), input_shape=[100, 2]))  \n",
    "model.add(Bidirectional(LSTM(30)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer = 'adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)   \n",
    "Y = np.array(Y)\n",
    "history = model.fit(X, Y, epochs=100, verbose=1, validation_split= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "plt.plot(history.history['loss'], 'b-', label='loss')  \n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')  \n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X[2560:], Y[2560:])\n",
    "prediction = model.predict(X[2560:2560 + 5])\n",
    "for i in range(5):\n",
    "    print(Y[2560 + i], '\\t', prediction[i][0], '\\tdiff:',\n",
    "          abs(prediction[i][0] - Y[2560 + i]))\n",
    "\n",
    "prediction = model.predict(X[2560:])\n",
    "cnt = 0\n",
    "for i in range(len(prediction)):\n",
    "    if abs(prediction[i][0] - Y[2560 + i]) > 0.04:\n",
    "        cnt += 1\n",
    "print('correctness:', (440 - cnt) / 440 * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "geographic-garbage",
   "metadata": {},
   "source": [
    "#### 개체명 인식\n",
    "\n",
    "    문장 내에 포함된 어떤 단어가 인물, 장소, 날짜 등을 의미하는 단어인지 인식하는 것\n",
    "    개체명 인식은 챗봇에서 문장을 정확하게 해석하기 위해 반드시 해야 하는 전처리 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split  #데이터 분할\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#학습파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []  #\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            # 만약 첫 번째 값이 ; 이 참이며, 두번째 줄의 첫 번째 값도 $ 이 맞으면,\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []  #빈 리스트를 생성\n",
    "                \n",
    "            #두 번째 조건, 여기는 그냥 넘어가야 하니 위 조건을 다시 가져와 넘겨준다.\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "                \n",
    "            #세 번째 조건, 만약 첫 번째 줄이 띄어쓰기라면 this_sent를 sents에 저장\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "                \n",
    "            #위 조건이 다 아니라면 즉, 분석 결과들이라면\n",
    "             #이 값들을 this_sent에 저장\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "\n",
    "corpus = read_file('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장과 테그를 저장할 빈 리스트 생성\n",
    "sentences, tags = [], []\n",
    "#위에서 추출한 corpus 데이터셋 한줄 씩 t에 할당\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    #t에 할당된 corpus의 데이를 또 나눠 w에 할당\n",
    "    for w in t:\n",
    "        #그 중 두 번째 텍스트와 마지막 BIO 테그를 동시에 tagged_sentence에 저장\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        #각각 따로 저장\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    #위에서 저장한 객체들을 하나로 묶어준다.\n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"2번째 샘플 문장 시퀀스 : \\n\", sentences[1])\n",
    "print(\"2번째 샘플 bio 태그 : \\n\", tags[1])\n",
    "print(\"샘플 문장 시퀀스 최대 길이 : \", max(len(l) for l in sentences))\n",
    "print(\"샘플 문장 시퀀스 평균 길이 : \", (sum(map(len, sentences)) / len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 정의\n",
    "#미리 인덱싱하지 않은 단어들은 OOV로 지정\n",
    "sent_tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "#위의 설정대로 문자를 입력받아 리스트의 형태로 변환\n",
    "sent_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#테그 정보는 소문자로 변환하지 않도록 셋팅\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False)\n",
    "#위의 설정대로 문자를 입력받아 리스트의 형태로 변환\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "#문장의 수\n",
    "vocab_size = len(sent_tokenizer.word_index) + 1\n",
    "#태그의 수\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"단어 사전 크기 : \", vocab_size)\n",
    "print(\"BIO 태그 사전 크기 : \", tag_size)\n",
    "\n",
    "#학습용 단어 시퀀스 생성 - texts_to_sequences()는 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환\n",
    "x_train = sent_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "print(x_train[1])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = sent_tokenizer.index_word  #시퀀스 인덱스를 단어로 변환하기 위해 사용\n",
    "index_to_ner = tag_tokenizer.index_word  #시퀀스 인덱스를 NER로 변환하기 위해 사용\n",
    "#print(index_to_word)\n",
    "#print(index_to_ner)\n",
    "index_to_ner[0] = 'PAD'  #첫 번째 값이 의미 없는 'O' 이기에 PAD처리\n",
    "max_len = 40\n",
    "print(\"before : \", x_train[1])\n",
    "print(\"before : \", y_train[1])\n",
    "#아래 함수는 길이가 제각기 다른 데이터 셋에 사용되며, padding을 통해 길이를 일정하게 만들어줍니다.\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,\n",
    "                                               padding='post',\n",
    "                                               maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train,\n",
    "                                               padding='post',\n",
    "                                               maxlen=max_len)\n",
    "print(\"after :\", x_train[1])\n",
    "print(\"after :\", y_train[1])\n",
    "\n",
    "#학습 데이터와 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "#출력 데이터를 원-핫 인코딩\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "print(\"one-hot / y_train : \", y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 정의 BI-LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=30,\n",
    "              input_length=max_len,\n",
    "              mask_zero=True))\n",
    "\n",
    "# 양방향 lstm 모델을 정의할 때 정방향, 역방향 lstm 계층에 모든 출력값을 연결해야 하기 때문에 \n",
    "# return_sequences 인자를 반드시 True\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        LSTM(200, return_sequences=True, dropout=0.50,\n",
    "             recurrent_dropout=0.25)))\n",
    "\n",
    "#또한 Dense 계층이 3차원 텐서를 입력받을 수 있게 확장해야 하므로 TimeDistributed()를 사용해야 합니다.\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(\"평가결과 : \", model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    #각 데이터에 대한 분리\n",
    "    for sequence in sequences:\n",
    "        temp = []\n",
    "        #각 데이터에 대한 패딩 처리된 태그별 확률 분리\n",
    "        for pred in sequence:\n",
    "            #분리한 값 중 제일 큰 인덱스를 pred_index에 저장\n",
    "            pred_index = np.argmax(pred)\n",
    "            #print(pred_index)\n",
    "            #인덱스에서 다시 NER로 변환하며 앞서 'PAD'처리한 부분을 다시 원래의 'O'로 변환합니다.\n",
    "            temp.append(index_to_ner[pred_index].replace('PAD', 'O'))\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "y_predicted = model.predict(x_test)\n",
    "#(711,40)->model->(711,40,8(각 NER객체에 대한 확률))\n",
    "#print(y_predicted)\n",
    "\n",
    "#비교를 위해서\n",
    "pred_tags = sequences_to_tag(y_predicted)  #예측된 값에 대한 NER\n",
    "test_tags = sequences_to_tag(y_test)  #실제 값에 대한 NER\n",
    "\n",
    "#F1 스코어 계산 과정\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score : {:.1%}\".format(f1_score(test_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#새로운 유형의 문장 NER예측\n",
    "word_to_index = sent_tokenizer.word_index  #각 단어들을 인덱스로 전환\n",
    "new_sentence = \"삼성전자 주가는 오늘 스마트폰 출시로 올라갔다.\".split()\n",
    "new_x = []\n",
    "for w in new_sentence:\n",
    "    try:\n",
    "        #get(key,'디폴트')는 key에 대한 value를 반환, 이때 존재하지 않는키면 KeyError를 불러일으킴\n",
    "        #이때 디폴트값이 정해진 경우라면 그 값을 반환  #여기서 1이란 값은 'OOV' 입니다.\n",
    "        new_x.append(word_to_index.get(w, 1))\n",
    "        \n",
    "    except KeyError:\n",
    "        new_x.append(word_to_index['OOV'])\n",
    "\n",
    "#위의 삼성전자 예시를 쪼개서 인덱싱한 결과를 보여줌\n",
    "print(\"새로운 유형의 시퀀스 : \", new_x)\n",
    "\n",
    "#패딩 처리\n",
    "new_padded_seqs = preprocessing.sequence.pad_sequences([new_x],\n",
    "                                                       padding='post',\n",
    "                                                       value=0,\n",
    "                                                       maxlen=max_len)\n",
    "\n",
    "#NER 예측\n",
    "p = model.predict(np.array([new_padded_seqs[0]]))\n",
    "p = np.argmax(p, axis=1)\n",
    "\n",
    "#출력 결과 표시\n",
    "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-argument",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-omaha",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-glory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
