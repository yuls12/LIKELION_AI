{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-norwegian",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bored-brief",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:21:39.966316Z",
     "start_time": "2021-04-20T04:21:39.953313Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran  \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "furnished-energy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:21:41.084491Z",
     "start_time": "2021-04-20T04:21:40.605571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 말뭉치 데이터 읽기 시작\n",
      "200000\n",
      "1) 말뭉치 데이터 읽기 완료:  0.45456957817077637\n"
     ]
    }
   ],
   "source": [
    "# 네이버 영화 리뷰 데이터 읽어옴\n",
    "def read_review_data(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]  # header 제거\n",
    "    return data\n",
    "\n",
    "\n",
    "# 측정 시작\n",
    "start = time.time()\n",
    "\n",
    "# 리뷰 파일 읽어오기\n",
    "print('1) 말뭉치 데이터 읽기 시작')\n",
    "review_data = read_review_data('data/ratings.txt')\n",
    "print(len(review_data))  # 리뷰 데이터 전체 개수\n",
    "print('1) 말뭉치 데이터 읽기 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-character",
   "metadata": {},
   "source": [
    "    ① 네이버 영화 리뷰 말뭉치 파일(ratings.txt)을 읽어와 리스트로 변환하는 함수입니다. \n",
    "    ratings.txt 파일은 라인마다 Tab 을 사용해서 id, document, label 컬럼으로 데이터가 구분되어있습니다. \n",
    "    따라서  read_review_data() 함수는 리뷰 데이터를 각 라인 별로 읽어와 \\t를 기준으로 데이터를 분리합니다.  \n",
    "    그 후 첫 번째 행의 헤더를 제거하고 리뷰 데이터만 반환합니다.\n",
    "\n",
    "    ①에서 정의한 read_review_data()함수를 호출해 현재 경로에 있는 ratings.txt 파일을 리스트 형태로 읽어옵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "superior-spell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:24:24.608533Z",
     "start_time": "2021-04-20T04:21:43.370941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) 형태소에서 명사만 추출 시작\n",
      "2) 형태소에서 명사만 추출 완료:  163.98976731300354\n"
     ]
    }
   ],
   "source": [
    "# 문장단위로 명사만 추출해 학습 입력 데이터로 만듬\n",
    "print('2) 형태소에서 명사만 추출 시작')\n",
    "komoran = Komoran()\n",
    "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
    "print('2) 형태소에서 명사만 추출 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-rugby",
   "metadata": {},
   "source": [
    "    ④ Komoran 형태소 분석기를 이용해 불러온 리뷰 데이터에서 문장별로 명사만 추출합니다.  \n",
    "    여기서 sentence[1]은 rating.txt 파일에서 document 컬럼의 데이터를 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "authorized-command",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:25:06.973067Z",
     "start_time": "2021-04-20T04:24:50.486771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) word2vec 모델 학습 시작\n",
      "3) word2vec 모델 학습 완료:  16.478259563446045\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# word2vec 모델 학습\n",
    "print('3) word2vec 모델 학습 시작')\n",
    "model = Word2Vec(sentences=docs, vector_size=200, window=4, min_count=2, sg=1)\n",
    "\n",
    "print('3) word2vec 모델 학습 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-mileage",
   "metadata": {},
   "source": [
    "    ⑤ ④에서 추출한 명사 리스트로 Word2Vec모델을 학습시킵니다. \n",
    "    여기서 Word2Vec()의 주요 하이퍼파라미터는 다음과 같습니다.\n",
    "    \n",
    "    vector_size의 경우 10만개의 데이터의 경우 100개의 vector_size가 적절하다.\n",
    "    window는 실험적으로 경험에 의한 값을 많이 사용한다.\n",
    "    min_count의 경우에는 2를 대체로 사용한다.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regulated-orange",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:25:08.852760Z",
     "start_time": "2021-04-20T04:25:08.562294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) 학습된 모델 저장 시작\n",
      "4) 학습된 모델 저장 완료:  0.2729966640472412\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 모델 저장\n",
    "print('4) 학습된 모델 저장 시작')\n",
    "model.save('data/nvmc.model')\n",
    "print('4) 학습된 모델 저장 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-director",
   "metadata": {},
   "source": [
    "    ⑥ 모델을 현재 디렉터리에 ‘nvmc.model’ 이란 이름으로 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regular-mercury",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:25:11.847677Z",
     "start_time": "2021-04-20T04:25:11.826585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_count :  200000\n",
      "corpus_total_words :  1076896\n"
     ]
    }
   ],
   "source": [
    "# 학습된 말뭉치 개수, 코퍼스 내 전체 단어 개수\n",
    "print(\"corpus_count : \", model.corpus_count)\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-murder",
   "metadata": {},
   "source": [
    "    주요 실행 시점마다 계산 시간을 측정했으며, \n",
    "    전처리 과정으로 리뷰 데이터 20만 개에서 명사를 추출하는데 약 131초 정도 소요되었고, \n",
    "    약 100만개 정도의 단어 임베딩을 처리할 수 있는 Word2Vec 모델을 학습하고 저장하는 데 약 19초 정도 소요되었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-acting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "devoted-mathematics",
   "metadata": {},
   "source": [
    "##### 단어 임베딩된 값과 벡터 공간상의 유사한 단어들을 확인하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abandoned-joyce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T02:04:05.160416Z",
     "start_time": "2021-04-21T02:04:04.835993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_total_words :  1076896\n",
      "사랑 :  [-0.09239019 -0.6113379   0.20210013  0.03807115 -0.20520669 -0.1487556\n",
      " -0.269031    0.13204677 -0.04312486  0.40844262 -0.13141038 -0.07152593\n",
      "  0.09941676 -0.19877242 -0.15639223  0.5141167  -0.03041169 -0.37351817\n",
      " -0.21048227 -0.48086122  0.3776593   0.13476379  0.12408568  0.10856127\n",
      " -0.16274454  0.00927417  0.03441136 -0.11233966 -0.16343175 -0.16550887\n",
      "  0.14632772  0.06695759  0.41403937 -0.08960392  0.14960782  0.20262213\n",
      "  0.08305626 -0.07944804 -0.35352397 -0.2742586  -0.20479113  0.1189067\n",
      " -0.11226966 -0.33121994  0.45288533  0.24256226  0.03242411 -0.1332587\n",
      "  0.4911213  -0.03512859  0.11811792 -0.14201482  0.03427875  0.02912429\n",
      " -0.24255635 -0.20937502  0.03123922 -0.28391117 -0.1790119   0.1610645\n",
      " -0.05075287  0.11262257 -0.29918575  0.16835694 -0.00819165  0.08042238\n",
      " -0.40971318  0.03663733 -0.11486392  0.62863564  0.21270254 -0.05054173\n",
      "  0.3662721   0.18001905  0.2973186  -0.0118237   0.40479502 -0.16692252\n",
      " -0.22196138 -0.37877998  0.10394074  0.11519604 -0.11613383  0.32355407\n",
      " -0.50043607 -0.24197876 -0.15775733  0.24872956  0.05038493  0.3200847\n",
      "  0.3507718   0.03893377  0.1520575   0.09258208  0.5357681  -0.0377298\n",
      "  0.10702368  0.14912114 -0.41195706  0.08657562 -0.5582445   0.5772963\n",
      "  0.03912506  0.1651293  -0.13685861 -0.5487666  -0.04381581  0.33140117\n",
      " -0.13675669 -0.19961217 -0.07700109 -0.39473048  0.2384077   0.15168649\n",
      "  0.23906791 -0.5750436   0.07251248 -0.2527752   0.18064255 -0.10744911\n",
      "  0.03478227 -0.07927541 -0.23476736 -0.17776729  0.19790967  0.38081077\n",
      " -0.34264168 -0.11112273 -0.41542035  0.07692319 -0.30754063  0.5384902\n",
      "  0.08048618  0.46733037 -0.04482367  0.14549996  0.10053609  0.14793956\n",
      " -0.138763   -0.01665018  0.23701449  0.07046637 -0.34188205  0.14276297\n",
      " -0.14579828 -0.07841002 -0.19507773 -0.21080017  0.25147337 -0.19857435\n",
      " -0.09727465 -0.5024369   0.38251072 -0.42779806 -0.14092849 -0.19871667\n",
      "  0.32671386  0.02038493  0.20580424  0.12072019  0.15187624  0.02156256\n",
      "  0.03749714 -0.07651381 -0.04375771  0.4737273  -0.09686487 -0.18780346\n",
      " -0.16820458  0.08879896 -0.55981    -0.07382464  0.28191322 -0.61636966\n",
      "  0.06755722  0.00280964  0.1939964  -0.20587254  0.30470842  0.19681047\n",
      "  0.11058792  0.02148535  0.02824998 -0.07957825 -0.19877693 -0.30780932\n",
      " -0.04710633 -0.187867    0.49444762 -0.1898982   0.19022804 -0.17440163\n",
      " -0.3176816   0.12923914  0.25129145  0.04180132 -0.04961213 -0.18865043\n",
      " -0.23316386  0.07830597]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 모델 로딩\n",
    "model = Word2Vec.load('data/nvmc.model')  # ① 앞서 생성한 모델 파일을 읽어와 Word2Vec 객체를 생성합니다.\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)\n",
    "\n",
    "# '사랑'이란 단어로 생성한 단어 임베딩 벡터\n",
    "print('사랑 : ', model.wv['사랑'])  # 모델을 학습할 때 설정한 size 하이퍼파라미터만큼 단어 임베딩 벡터 차원 크기가 결정됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scientific-convert",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T04:25:22.651848Z",
     "start_time": "2021-04-20T04:25:22.625620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일요일 = 월요일\t 0.92875725\n",
      "안성기 = 배우\t 0.715711\n",
      "대기업 = 삼성\t 0.8698282\n",
      "일요일 != 삼성\t 0.63510627\n",
      "히어로 != 삼성\t 0.42573237\n",
      "[('정려원', 0.9387677311897278), ('정재영', 0.9337273240089417), ('재발견', 0.9329094290733337), ('설경구', 0.9327645301818848), ('지진희', 0.9311741590499878)]\n",
      "[('엑스맨', 0.8029845952987671), ('포터', 0.7887096405029297), ('반지의 제왕', 0.7793396711349487), ('해리', 0.7742770314216614), ('에이리언', 0.7708970308303833)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 유사도 계산\n",
    "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))  \n",
    "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))  \n",
    "print(\"대기업 = 삼성\\t\", model.wv.similarity(w1='대기업', w2='삼성'))  \n",
    "print(\"일요일 != 삼성\\t\", model.wv.similarity(w1='일요일', w2='삼성'))  \n",
    "print(\"히어로 != 삼성\\t\", model.wv.similarity(w1='히어로', w2='삼성'))\n",
    "\n",
    "# 가장 유사한 단어 추출\n",
    "print(model.wv.most_similar(\"안성기\", topn=5))  \n",
    "print(model.wv.most_similar(\"시리즈\", topn=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-audience",
   "metadata": {},
   "source": [
    "    Ⓒ gensim 패키지의 model.wv.most_similarity() 함수를 호출할 경우 인자로 사용한 단어와 가장 유사한 단어를 리스트로 반환해 줍니다. \n",
    "    즉, 벡터 공간에서 가장 가까운 거리에  있는 단어들을 반환합니다. \n",
    "    topn 인자는 반환되는 유사한 단어 수를 의미하며, 예제에서는 5개 까지 유사한 단어를반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-poker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "together-ontario",
   "metadata": {},
   "source": [
    "#### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어절 단위 n-gram\n",
    "def word_ngram(bow, num_gram):\n",
    "    text = tuple(bow)\n",
    "    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\n",
    "    return tuple(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-burst",
   "metadata": {},
   "source": [
    "    ① 어절 단위로 n-gram 토큰을 추출하는 함수입니다. 추출된 토큰들은 튜플 형태로 반환됩니다.\n",
    "    이전 예시처럼 슬라이싱을 이용해 문장을 어절 단위로 n개씩 끊어서 토큰을 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산\n",
    "def similarity(doc1, doc2):  \n",
    "    cnt = 0\n",
    "    for token in doc1:\n",
    "        if token in doc2:  \n",
    "            cnt = cnt + 1\n",
    "            \n",
    "    return cnt/len(doc1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-silver",
   "metadata": {},
   "source": [
    "    Ⓒ doc1의 토큰이 doc2의 토큰과 얼마나 동일한지 횟수를 카운트 합니다. \n",
    "    카운트된 값을  doc1의 전체 토큰 수로 나누면 유사도가 계산됩니다. \n",
    "    이때 결과가 1.0에 가까울수록 doc1과 유사해 집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
    "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
    "sentence3 = '나는 맛있는 밥을 뉴턴 선생님과 함께 먹었습니다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석기에서 명사(단어) 추출\n",
    "komoran = Komoran()\n",
    "bow1 = komoran,nouns(sentences1)\n",
    "bow2 = komoran,nouns(sentences2)\n",
    "bow3 = komoran,nouns(sentences3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 n-gram 토큰 추출\n",
    "doc1 = word_ngram(bow1, 2) #2-gram 방식으로  추출\n",
    "doc2 = word_ngram(bow2, 2)  \n",
    "doc3 = word_ngram(bow3, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-tomorrow",
   "metadata": {},
   "source": [
    "    word_ngram() 함수를 이용해 정의된 문장에서 명사를 리스트 형태로 추출합니다.\n",
    "    여기서 word_ngram() 함수의 num_gram 인자에 2를 입력했으므로 2-gram 방식으로 토큰을 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "#추출된 n-gram 토큰 출력  \n",
    "print(doc1)  \n",
    "print(doc2)  \n",
    "print(doc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#유사도 계산\n",
    "r1 = similarity(doc1, doc2)  \n",
    "r2 = similarity(doc3, doc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#계산된 유사도 출력  \n",
    "print(r1)  \n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-insert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "provincial-venue",
   "metadata": {},
   "source": [
    "#### 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-certificate",
   "metadata": {},
   "source": [
    "    ① 코사인 유사도를 계산하는 함수입니다. \n",
    "    코사인 유사도 계산에는 넘파이에서 제공하는 벡터 내적을 계산하는 함수와 노름(𝑛𝑜𝑟𝑚)을 계산하는 함수를 이용합니다.\n",
    "    넘파이의 dot()함수는 인자로 들어온 2개의 넘파이 배열을 내적곱(𝑑𝑜𝑡 𝑝𝑟𝑜𝑑𝑢𝑐𝑡)합니다. \n",
    "    넘파이에서는 norm()함수를 제공합니다.\n",
    "    여기서 노름은 벡터의 크기를 나타내는 수학 용어입니다. \n",
    "    벡터의 노름에는 여러가지 종류가 있지만 코사인 유사도에서는 L2 노름(유클리드 노름)을 주로 사용합니다. \n",
    "    L2 노름의 수식은 아래와 같습니다.\n",
    "    \n",
    "    코사인 유사도 수식의 분모에 있는 벡터의 크기를 계산하는 부분과 동일하기 때문에 벡터의 크기 계산에 넘파이 norm() 함수를 사용합니다.\n",
    "    norm()함수의 인자로 들어온 넘파이 배열의 노름 크기를 계산합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDM 만들기\n",
    "def make_term_doc_mat(sentence_bow, word_dics):  \n",
    "    freq_mat = {}\n",
    "    for word in word_dics:  \n",
    "        freq_mat[word] = 0\n",
    "\n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:  \n",
    "            freq_mat[word] += 1\n",
    "\n",
    "    return freq_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-venice",
   "metadata": {},
   "source": [
    "    Ⓒ 비교 문장에서 추출한 단어 사전을 기준으로 문장에 해당 단어들이 얼마나 포함되어 있는지 나타내는 \n",
    "    단어 문서 행렬 TDM을 만들어주는 함수 입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 벡터 만들기\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-creativity",
   "metadata": {},
   "source": [
    "    ③ 단어 문서 행렬에서 표현된 토큰들의 출현 빈도 데이터를 벡터로 만들어 주는 함수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 정의\n",
    "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'  \n",
    "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'  \n",
    "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
    "\n",
    "# 헝태소분석기를 이용해 단어 묶음 리스트 생성\n",
    "komoran = Komoran()\n",
    "bow1 = komoran.nouns(sentence1)  \n",
    "bow2 = komoran.nouns(sentence2)  \n",
    "bow3 = komoran.nouns(sentence3)\n",
    "\n",
    "# 단어 묶음 리스트를 하나로 합침\n",
    "bow = bow1 + bow2 + bow3\n",
    "\n",
    "# 하나로 합쳐진 단어 묶음 리스트에서 중복된 단어를 제거해 새로운 단어 사전 리스트를 구축\n",
    "word_dics = []  \n",
    "for token in bow:\n",
    "    if token not in word_dics:\n",
    "        word_dics.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 별 단어 문서 행렬 계산\n",
    "freq_list1 = make_term_doc_mat(bow1, word_dics)  \n",
    "freq_list2 = make_term_doc_mat(bow2, word_dics)  \n",
    "freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
    "\n",
    "# 각 문장마다 단어 문서 행렬 리스트를 만든 후 출력합니다.\n",
    "print(freq_list1)  \n",
    "print(freq_list2)  \n",
    "print(freq_list3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장마다 벡터를 생성해 넘파이 배열로 변환\n",
    "doc1 = np.array(make_vector(freq_list1))  \n",
    "doc2 = np.array(make_vector(freq_list2))  \n",
    "doc3 = np.array(make_vector(freq_list3))\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "r1 = cos_sim(doc1, doc2)  \n",
    "r2 = cos_sim(doc3, doc1)  \n",
    "\n",
    "print(r1)\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-modern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-tomato",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
